<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="./style.css">
    <title>Title</title>
    <style>
        * {
            margin: 0;
            padding: 0;
        }
    </style>
</head>
<body  topmargin="0"  leftmargin="0" marginwidth="0" marginheight="0">
    <header style="text-align: center;background-color:rgb(187, 187, 187); padding: 30px;"  >
        <h1>
            <p>Hyper‑MoA: Achieving High Quality and Parameter Efficiency in Few‑Shot Multi‑Speaker TTS</p>
        </h1>
        <h2>
            <p>
                Rakbeen Song, Kyungseok Oh, Chulwon Choi, Bonhwa Ku, Hanseok Ko
            </p>
        </h2>
        <h3>
            <!-- <p>test</p>-->
        </h3>
    </header>
    <main>
        <div class="setting-box">
            <h2>
                <p>Abstract</p>
            </h2>
            <p>Advances in text‑to‑speech (TTS) have enabled high‑quality, human‑like voice synthesis, yet synthesizing new speaker speech with only a few minutes of data remains challenging. To synthesize new speakers’ speech with only a few minutes of data, adapter‑based few‑shot learning has been proposed. This approach inserts lightweight modules into a pre‑trained multi‑speaker backbone, effectively preserving its quality while adding new voices with minimal parameter overhead. However, existing adapter‑based few‑shot learning methods support only single‑speaker adaptation—each new speaker requires its own adapter, leading to increased parameters. To overcome this limitation, we propose Hyper‑MoA, which integrates a Hyper‑network with a Mixture‑of‑Adapters framework to enable multi‑speaker adaptation within a single module. Hyper‑MoA operates in a speaker‑specific manner, allowing one module to accommodate any number of new speakers without increasing model size. During adaptation, it learns shared representations through speaker interactions, alleviating performance degradation caused by limited data in few‑shot scenarios. Experiments under varied speaker counts, data amounts, and adapter compression settings demonstrate that Hyper‑MoA consistently outperforms conventional MoA while delivering significant parameter savings—confirming its effectiveness and efficiency for few‑shot multi‑speaker TTS adaptation.</P>
        </div>

        <div class="setting-box">
            <h2>
                <p>Setting</p>
            </h2>
            <li>
                <b>Dataset</b>: VCTK
            </li>
            <li>
                <b>Vocoder</b>: HifiGAN
            </li>
        </div>

        <div class="setting-box">
            <h2>
                <p>
                    Performance comparison on few-shot adaptation TTS
                </p>
            </h2>
            <li>
                <b>Key comparison point 1</b>: Performance difference between single-speaker adaptation using conventional MoA and multi-speaker adaptation using Hyper-MoA
            </li>
            <li>
                <b>Key comparison point 2</b>: Performance improvement with an increasing number of total speakers in multi-speaker adaptation using Hyper-MoA (multi_2 → multi_6 → multi_10)
            </li>
        </div>

    </main>
</body>
</html>
